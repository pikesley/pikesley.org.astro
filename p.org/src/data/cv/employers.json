[
  {
    "name": "Cervest",
    "url": "//cervest.earth/",
    "location": "London",
    "role": "Senior Infrastructure Engineer",
    "dates": {
      "start": "October 2021",
      "end": "present"
    },
    "headline": "Cervest is architecting a new era of climate intelligence.\n"
  },
  {
    "name": "Demand Logic",
    "url": "//www.demandlogic.co.uk/",
    "location": "London",
    "role": "Senior Infrastructure Engineer",
    "dates": {
      "start": "February 2019",
      "end": "September 2021"
    },
    "headline": "Demand Logic is a software tool which provides actionable intelligence to property managers and building contractors. It is intended to deliver quantifiable benefits in a short space of time and to make the management of buildings easier.\n",
    "profile": "I joined Demand Logic at the beginning of a project to shift their infrastructure away from [Rackspace Cloud](//www.rackspace.com/en-gb/cloud) and into [Google Cloud](//cloud.google.com). As part of this, I introduced [Terraform](//www.terraform.io), which now describes every GCP resource at DL in a repeatable way.I was also responsible for introducing [Concourse CI](//concourse-ci.org) (to replace Jenkins) and have built extensive (Python) tooling around this to, among other things, bring the platform towards Continuous Delivery.\n"
  },
  {
    "name": "OpenCorporates",
    "url": "//opencorporates.com/",
    "location": "London",
    "role": "Backend Engineer",
    "dates": {
      "start": "March 2018",
      "end": "February 2019"
    },
    "headline": "OpenCorporates is the largest open database of companies and company data in the world, with in excess of 100 million companies in a similarly large number of jurisdictions.\n",
    "profile": "I primarily worked as a Ruby dev at OpenCorporates, maintaining their Rails apps, although I also turned my hand to occasional bits of SysAdmin-ish work when required. I was also heavily involved in a project to map corporate networks, initially using [Neo4j](//neo4j.com) and later [TigerGraph](//www.tigergraph.com).\n\nI was eventually lured away by the prospect of getting back into more straight-up Ops work.\n"
  },
  {
    "name": "Moo",
    "url": "//moo.com",
    "location": "London",
    "role": "Operations Engineer",
    "dates": {
      "start": "January 2017",
      "end": "March 2018"
    },
    "headline": "Moo is a print-on-demand company printing business cards, postcards, stickers and other material.\n",
    "profile": "I joined Moo at the beginning of a massive project to migrate from hosted tin to AWS. This involved jumping in at the deep-end with Terraform and Ansible, and was pretty much all I worked on until July 2017, when we pulled the trigger on a remarkably smooth cut-over.\n\nThe rest of the summer was spent on cleaning-up some things we'd kicked down the road during the migration project, and since September I've been on a project to build a Kubernetes-based self-service platform.\n"
  },
  {
    "name": "The Open Data Institute",
    "url": "//theodi.org",
    "location": "London",
    "role": "Head Of Robots",
    "dates": {
      "start": "January 2013",
      "end": "December 2016"
    },
    "headline": "The ODI connects, equips and inspires people around the world to innovate with data.\n",
    "profile": "My role at the ODI encompassed many things - initially, leading on the building and maintenance of all of the [test-driven Chef infrastructure](//github.com/theodi/chef-skellington) for the ODI's first round of core tools, primarily [Open Data Certificates](//toolbox.theodi.org/tools/certificates/) and [CSV Lint](//toolbox.theodi.org/tools/csvlint/), and our [adoption of GDS's CMS suite](//skillsmatter.com/skillscasts/5181-adventures-in-early-adoption-of-open-source-code).\n\nSubsequent highlights include:\n\n  * Gaining a comprehensive grasp of HTTP and REST (mainly from working under the guidance of [Jeni Tennison](//theodi.org/team/jeni-tennison))\n  * Learning to love TDD, primarily through heavy exposure to RSpec and Cucumber\n  * Embracing the Github -> Travis -> Heroku continuous deployment chain\n  * Building front-end apps (e.g. the TfL [Train Data Demonstrator](//goingunderground.herokuapp.com/)) and so getting a handle on Bootstrap, JavaScript, Sass and D3\n  * Providing technical mentoring for one of our PhD students from the [WDAqua](//wdaqua.eu/) programme\n  * Occasional speaking engagements, primarily on the subject of Working In The Open\n\n*Everything* we ever built at the ODI has been built in the open and all of our code is [up on Github](//github.com/theodi).\n"
  },
  {
    "name": "AMEE UK",
    "url": "//amee.com",
    "location": "London",
    "role": "DevOps Engineer",
    "dates": {
      "start": "June 2011",
      "end": "January 2013"
    },
    "headline": "AMEE is a start-up whose mission is to measure the carbon footprint of everything on the planet.\n",
    "profile": "Working closely with AMEE's relatively small team of Java and Ruby devs, I was responsible for:\n\n* Putting in a huge amount of Chef plumbing (running off of [AMEE's own Chef\n  server](//www.amee.com/blog/2012/02/22/building-a-chef-server-from-scratch/)) -\n  AMEE's config management previously consisted of a handful of bash scripts\n* Deploying and configuring Splunk\n* Migrating several of AMEE's legacy apps from leased iron in a DC to AWS\n\nalongside the usual SysAdmin work of backup-and-restore, capacity planning, etc.\n"
  },
  {
    "name": "VisualDNA",
    "url": "//visualdna.com",
    "location": "London",
    "role": "Systems Administrator",
    "dates": {
      "start": "August 2009",
      "end": "June 2011"
    },
    "headline": "VisualDNA is a dynamic startup based in Soho. The company generates profiles for users\nthrough the use of visual quizzes, working with clients including the LA Times,\nmatch.com and the Daily Mirror.\n",
    "profile": "My role at VisualDNA was pretty much DevOps before I knew that DevOps was even a thing\n– as a busy startup with diverse client projects there were often multiple\ndeploys per day, meaning I had to work very closely with the developers to\nmake sure we were all on the same page.\n\nThis close working relationship was particularly fruitful during the gradual\ntransfer of many of VisualDNA’s core services from the legacy platform (a\ncouple of racks of Linux boxes in a London datacentre) to Amazon Web Services.\nThe back-and-forth between myself and the development team was invaluable as\nwe iterated through various combinations of EC2 Instance Types to find the\nsetup that best fitted our requirements.\n\nThe transfer to AWS accelerated rapidly during 2011; the setup I left them with included:\n\n* A 16-node Cassandra cluster\n* A 6-node Hadoop cluster\n* Several groups of Elastic-Load-Balanced web servers\n\nThis platform is still in production use, including an all-new Quiz Engine\nwhich as far as I know is still performing extremely well.\n"
  },
  {
    "name": "Rex Features",
    "url": "//rexfeatures.com",
    "location": "London",
    "role": "Systems Administrator",
    "dates": {
      "start": "August 2003",
      "end": "August 2009"
    },
    "headline": "Rex Features is Britain’s leading independent photographic press agency and\npicture library. Rex supplies a daily service of news, celebrity, features,\nand stock photos to all national newspapers, magazines, TV, web and other\nmedia in the UK and in more than 30 countries worldwide.\n",
    "profile": "My work at Rex covered the usual gamut of Sysadmin tasks, including: backup\nand recovery, webserver administration, DNS management, plenty of scripting\n(mostly in bash), patching servers, and writing and maintaining documentation.\nThere was also some SQL Server admin, and a certain amount of desktop support –\nRex is a company of ~80 employees, supported by an IT department of four.\nAll new server hardware passed through my hands for installation and configuration.\n\nWhen I joined Rex, the IT department consisted of two very busy people. The\nIT infrastructure had been growing rapidly, deployment had happened on a\nseemingly ad-hoc basis, and documentation was fairly sparse. My initial tasks included:\n\n* Installing a CVS server (yes, this was 2003) and gathering code and scripts into it\n* Rolling out the Amanda backup system and setting up a proper backup and recovery scheme\n* Getting the RT ticketing system up and running (we later moved to Jira)\n* Beginning the process of documenting everything in Twiki (we subsequently migrated to Mediawiki)\n\nSubsequently, I was directly involved in:\n\n* Setting up the Nagios network monitoring system\n* Migrating the internal mail from Novell to Exchange, and later outsourcing\n  this function to Cobweb’s hosted Exchange platform\n* Configuring VPNs between Rex’s headquarters and various locations – initially\n  using isakmpd on OpenBSD, and latterly on a Watchguard Firebox\n* Overseeing the transfer of Rex’s image data – 5 terabytes of jpegs at the\n  time of writing – from a cluster based on a number of FreeBSD servers to a\n  set of Network Appliance 3050 filers\n* Configuring and deploying Alteon load-balancers for the Rex website, which\n  gets ~1.5 million hits and shifts ~13 gigs of data a day\n* Migrating Rex’s code from CVS to Subversion\n* Specifying and documenting a “standard Rex server install” – except for a\n  handful of Windows servers, the whole of Rex’s server room and colo are\n  running FreeBSD, so the standard install is a set of common ports and a\n  number of scripts\n\nRex went on to acquire another picture agency in Los Angeles, which brought\nabout a project to integrate their image archive into Rex’s, modifying the\nserver software to enable them to use the Rex client application, and\ndeploying a new set of servers to support of all of this. The final setup\nconsisted of a redundant pair of Microsoft SQL servers, a set of NetApp\nfilers and shelves, and a group of FreeBSD servers (running apache and mod_perl)\nserving up three websites and a range of internal webservices, all sitting\nbehind a pair of Alteon load-balancers.\n"
  },
  {
    "name": "Empower Interactive",
    "location": "London",
    "role": "Systems Administrator",
    "dates": {
      "start": "April 2000",
      "end": "July 2003"
    },
    "headline": "Empower Interactive was a telecoms software startup, founded in 2000 in the\nCity of London.\n",
    "profile": "Having joined Empower at its inception, my initial responsibilities were to\ndesign and implement the IT infrastructure necessary to support the operations\nof the fledgling business. This included: network planning; server acquisition\nand installation (various internal servers, external mail server, firewall, etc);\ndeploying a backup and recovery scheme; managing the website; and a great deal\nof user education. I also got involved in many other aspects of the business –\nthis was a tiny start-up, so I found myself doing testing, writing user manuals\nfor Empower’s products, and even doing a little Java.\n\nI was solely responsible for supporting this infrastructure for the first year,\nuntil the business expanded to the extent that further IT staff were required.\nI was asked to set up an IT department, and recruited another Sysadmin who\nspecialised in Windows; the team continued to expand over the following years.\nI gained experience with Solaris 8, HP-UX and qualified as an Oracle administrator\nin order to install some of Empower’s products onto carrier-grade hardware;\nI also assisted with deploying the hardware into telcos.\n\nEmpower unfortunately ceased trading in November 2006.\n"
  }
]
